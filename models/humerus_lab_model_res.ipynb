{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337c0af3",
   "metadata": {},
   "source": [
    "# 3D CAE Model Humerus Labelmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb87400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import h5py\n",
    "from skimage.transform import downscale_local_mean,resize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score,calinski_harabasz_score,davies_bouldin_score\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv3D,Conv3DTranspose,Dense,MaxPooling3D,UpSampling3D,Flatten,Dense,Reshape,GlobalAveragePooling3D,BatchNormalization,Dropout\n",
    "from keras.activations import relu\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from itkwidgets import view\n",
    "\n",
    "# My imports (se si modifica il file bisogna riavviare tutto il kernel)\n",
    "from utilities import plot_all_slices_notzero,plot_slices,dice_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec2b16-681e-4e71-98b9-c2563894c713",
   "metadata": {
    "tags": []
   },
   "source": [
    "st=time.time()\n",
    "with h5py.File(\"dataset_humerus_labelmap_res.hdf5\",\"r\") as f:\n",
    "    humerus_dataset = f.get(\"mydataset\")[:]\n",
    "print(f\"Tempo lettura dataset: {time.time()-st:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12724283-145f-4529-9e49-44d7198b54ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st=time.time()\n",
    "with h5py.File(\"dataset_humerus_labelmap_res_flipped.hdf5\",\"r\") as f:\n",
    "    humerus_dataset = f.get(\"mydataset\")[:]\n",
    "print(f\"Tempo lettura dataset: {time.time()-st:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76801a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "humerus_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4f755",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Downsampling\n",
    "\n",
    "Meglio inziare con un downsampling per evitare problemi di memoria inizialmente.\n",
    "\n",
    "Dataset: 288x210x393\\\n",
    "Padding a: 288x216x400\\\n",
    "Downsample per 4 a: 72x54x100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f47a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "humerus_dataset = np.pad(humerus_dataset,((0,0),(0,0),(3,3),(4,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef21f5-5284-48c4-84c9-5c20396609cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "humerus_dataset = resize(humerus_dataset,(humerus_dataset.shape[0],humerus_dataset.shape[1]/4,humerus_dataset.shape[2]/4,humerus_dataset.shape[3]/4),preserve_range=True,order=0,anti_aliasing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d200a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "humerus_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c03c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"resized_dataset_humerus_labelmap_res_flipped.hdf5\",\"w\") as f:\n",
    "    f.create_dataset(\"mydataset\",data=humerus_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3d15c",
   "metadata": {},
   "source": [
    "# Model downsampled\n",
    "\n",
    "Una volta importato il dataset possiamo iniziare ad occuparci della costruzione del modello. Partiamo da immagini downscalate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30aafab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st=time.time()\n",
    "with h5py.File(\"resized_dataset_humerus_labelmap_res_flipped.hdf5\",\"r\") as f:\n",
    "    humerus_dataset = f.get(\"mydataset\")[:]\n",
    "print(f\"Tempo lettura dataset: {time.time()-st:.2f} sec\")\n",
    "humerus_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d30b081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resized_dataset_exp = np.expand_dims(humerus_dataset,axis=-1)\n",
    "resized_dataset_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14031f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model downscaled con STRIDE (meglio)\n",
    "embedding_dim = 128\n",
    "inputs = keras.Input(shape=(resized_dataset_exp.shape[1],resized_dataset_exp.shape[2],resized_dataset_exp.shape[3],1))\n",
    "\n",
    "# Encoder\n",
    "x = Conv3D(filters=32,kernel_size=(3,3,3),padding=\"same\",name=\"conv1_1\")(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "x = Conv3D(filters=32,kernel_size=(3,3,3),strides=2,padding=\"same\",name=\"conv1_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = Conv3D(filters=64,kernel_size=(3,4,3),padding=\"valid\",name=\"conv2_1\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "x = Conv3D(filters=64,kernel_size=(3,3,3),strides=2,padding=\"same\",name=\"conv2_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = Conv3D(filters=128,kernel_size=(4,3,3),padding=\"valid\",name=\"conv3_1\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "x = Conv3D(filters=128,kernel_size=(3,3,3),strides=2,padding=\"same\",name=\"conv3_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = Conv3D(filters=128,kernel_size=(3,3,3),padding=\"valid\",name=\"conv4\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "\n",
    "# Bottleneck\n",
    "shape_before_encoding = x.shape[1:]\n",
    "x = Flatten(name=\"flatten\")(x) \n",
    "flattened_size = x.shape[1]\n",
    "x = Dropout(0.3)(x)\n",
    "embedding = Dense(embedding_dim,activation=\"relu\",name=\"embedding\")(x)\n",
    "x = Dropout(0.3)(embedding)\n",
    "x = Dense(flattened_size,activation=\"relu\",name=\"expanding\")(x)\n",
    "x = Reshape(shape_before_encoding,name=\"reshape\")(x)\n",
    "\n",
    "# Decoder\n",
    "x = Conv3DTranspose(filters=128,kernel_size=(3,3,3),padding=\"valid\",name=\"deconv4\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = Conv3DTranspose(filters=64,kernel_size=(3,3,3),padding=\"same\",name=\"deconv3_1\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "x = Conv3DTranspose(filters=64,kernel_size=(3,3,3),strides=2,padding=\"same\",name=\"deconv3_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = Conv3DTranspose(filters=32,kernel_size=(4,3,3),padding=\"valid\",name=\"deconv2_1\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "x = Conv3DTranspose(filters=32,kernel_size=(3,3,3),strides=2,padding=\"same\",name=\"deconv2_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = Conv3DTranspose(filters=16,kernel_size=(3,4,3),padding=\"valid\",name=\"deconv1_1\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "x = Conv3DTranspose(filters=16,kernel_size=(3,3,3),strides=2,padding=\"same\",name=\"deconv1_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "\n",
    "outputs = Conv3DTranspose(filters=1,kernel_size=(3,3,3),padding=\"same\",name=\"deconv0\",activation=\"sigmoid\")(x)\n",
    "\n",
    "model_downsample = keras.Model(inputs,outputs,name=\"model_downscaled\")\n",
    "model_downsample.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0b806-eee1-4a49-b3cb-0374eaec881d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"training_history/hum_loss.pkl\",\"rb\") as f1,open(\"training_history/hum_val_loss.pkl\",\"rb\") as f2,open(\"training_history/hum_dice.pkl\",\"rb\") as f3,open(\"training_history/hum_val_dice.pkl\",\"rb\") as f4:\n",
    "    tot_loss = pickle.load(f1)\n",
    "    tot_val_loss = pickle.load(f2)\n",
    "    tot_dice = pickle.load(f3)\n",
    "    tot_val_dice = pickle.load(f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258da0d-f3ea-47ed-af81-dc7c00e348b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tot_loss = []\n",
    "tot_val_loss = []\n",
    "tot_dice = []\n",
    "tot_val_dice = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f6c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_downsample.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[dice_coef]\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"models_weights/humerus_flipped_checkpoint.weights.h5\",\n",
    "    monitor=\"loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "history = model_downsample.fit(\n",
    "    x=resized_dataset_exp,\n",
    "    y=resized_dataset_exp,\n",
    "    callbacks=[checkpoint],\n",
    "    # validation_split=0.2,\n",
    "    batch_size=16,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fd281-b2b8-4151-a948-532bc81630b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_downsample.save_weights(\"models_weights/humerus_flipped_loss0084_250ep_noval.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c68cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Non memorizzo le ultime 50 epoche senza validation \n",
    "tot_loss += history.history['loss'] # Per appendere liste tra loro\n",
    "tot_val_loss+=history.history['val_loss']\n",
    "tot_dice+=history.history['dice_coef']\n",
    "tot_val_dice+=history.history['val_dice_coef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4929a6c-bc55-4b7f-baf9-a946809a3344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"training_history/hum_flip_loss.pkl\",\"wb\") as f1,open(\"training_history/hum_flip_val_loss.pkl\",\"wb\") as f2,open(\"training_history/hum_flip_dice.pkl\",\"wb\") as f3,open(\"training_history/hum_flip_val_dice.pkl\",\"wb\") as f4:\n",
    "    pickle.dump(tot_loss,f1)\n",
    "    pickle.dump(tot_val_loss,f2)\n",
    "    pickle.dump(tot_dice,f3)\n",
    "    pickle.dump(tot_val_dice,f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a60524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"Loss\"\n",
    "fig,ax = plt.subplots(2,1,figsize=(18,18))\n",
    "start = 0\n",
    "end = 200\n",
    "\n",
    "# Loss\n",
    "ax[0].plot(tot_loss[start:end+1],marker=\".\")\n",
    "ax[0].plot(tot_val_loss[start:end+1],marker=\".\")\n",
    "ax[0].set_title('Loss del Modello')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epoche')\n",
    "ax[0].set_xticks([x for x in range(0,end+1-start,5)])\n",
    "ax[0].set_xticklabels([x for x in range(start,end+1,5)])\n",
    "ax[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "ax[0].grid()\n",
    "\n",
    "# Dice coefficient\n",
    "ax[1].plot(tot_dice[start:end+1],marker=\".\")\n",
    "ax[1].plot(tot_val_dice[start:end+1],marker=\".\")\n",
    "ax[1].set_title('Dice Coefficient del Modello')\n",
    "ax[1].set_ylabel('Dice Coefficient')\n",
    "ax[1].set_xlabel('Epoche')\n",
    "ax[1].set_xticks([x for x in range(0,end+1-start,5)])\n",
    "ax[1].set_xticklabels([x for x in range(start,end+1,5)])\n",
    "ax[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "ax[1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f28ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76099b7-d250-43eb-95d1-8b39fc339a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_downsample.load_weights(\"models_weights/humerus_flipped_loss0138_50ep.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69970d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_ct = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ad64a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = model_downsample(np.expand_dims(resized_dataset_exp[pred_ct],axis=0))\n",
    "pred_squeezed = np.squeeze(pred)\n",
    "pred_squeezed_binary = np.where(pred_squeezed >= 0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3055f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dice_coef(np.float32(resized_dataset_exp[pred_ct]),pred_squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c318e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_slice = 60\n",
    "plot_slices(resized_dataset_exp[pred_ct],2,8,start_slice,title=\"Originale\",titlesize=40)\n",
    "plot_slices(pred_squeezed_binary,2,8,start_slice,title=\"Predizione Binaria\",titlesize=40)\n",
    "plot_slices(pred_squeezed,2,8,start_slice,title=\"Predizione\",titlesize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a4a8f-d5ef-49b4-b7ed-2e75d44eade4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carico i pesi 50 epoche\n",
    "model_downsample.load_weights(\"models_weights/humerus_loss0106_100ep.weights.h5\")\n",
    "pred1 = model_downsample(np.expand_dims(resized_dataset_exp[pred_ct],axis=0))\n",
    "pred1_squeezed = np.squeeze(pred1)\n",
    "pred1_squeezed_binary = np.where(pred1_squeezed >= 0.5,1,0)\n",
    "print(dice_coef(np.float32(resized_dataset_exp[pred_ct]),pred1_squeezed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59abbb0-1756-4e8f-b13f-b35f3210dbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carico i pesi 150 epoche\n",
    "model_downsample.load_weights(\"models_weights/humerus_loss0092_150ep.weights.h5\")\n",
    "pred2 = model_downsample(np.expand_dims(resized_dataset_exp[pred_ct],axis=0))\n",
    "pred2_squeezed = np.squeeze(pred2)\n",
    "pred2_squeezed_binary = np.where(pred2_squeezed >= 0.5,1,0)\n",
    "print(dice_coef(np.float32(resized_dataset_exp[pred_ct]),pred2_squeezed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec4fd6-c857-410c-b655-70e7f0ff3db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carico i pesi 200 epoche\n",
    "model_downsample.load_weights(\"models_weights/humerus_loss0085_200ep.weights.h5\")\n",
    "pred3 = model_downsample(np.expand_dims(resized_dataset_exp[pred_ct],axis=0))\n",
    "pred3_squeezed = np.squeeze(pred3)\n",
    "pred3_squeezed_binary = np.where(pred3_squeezed >= 0.5,1,0)\n",
    "print(dice_coef(np.float32(resized_dataset_exp[pred_ct]),pred3_squeezed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c44a99-e736-42f6-8082-a612665310d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_slice = 60\n",
    "plot_slices(resized_dataset_exp[pred_ct],1,8,start_slice,title=\"Originale\",titlesize=40)\n",
    "plot_slices(pred1_squeezed_binary,1,8,start_slice,title=\"Predizione dopo 100 epoche\",titlesize=40)\n",
    "plot_slices(pred2_squeezed_binary,1,8,start_slice,title=\"Predizione dopo 150 epoche\",titlesize=40)\n",
    "plot_slices(pred3_squeezed_binary,1,8,start_slice,title=\"Predizione dopo 200 epoche\",titlesize=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a453fd9",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee3570-5563-43c9-9f38-7f22e59c45a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_downsample.load_weights(\"models_weights/humerus_checkpoint.weights.h5\") # Il checkpoint contiene quello con loss minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c6590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_model = keras.Model(model_downsample.inputs,model_downsample.get_layer(\"embedding\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86543807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = feature_model.predict(resized_dataset_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6ba11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbdfc0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"processing/humerus_res_flip_features_250ep.pkl\",\"wb\") as f:\n",
    "    pickle.dump(features,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
